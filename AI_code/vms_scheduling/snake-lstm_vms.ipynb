{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b169ee5-9af7-43dd-ae0f-f7c7f7e10bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import LSTM, Dense,Dropout,Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b79ea76-1981-4712-9bae-1673399c9f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 20 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   TaskID              2000 non-null   int64  \n",
      " 1   TaskFileSize        2000 non-null   int64  \n",
      " 2   TaskOutputFileSize  2000 non-null   int64  \n",
      " 3   TaskFileLength      2000 non-null   int64  \n",
      " 4   UserLatitude        2000 non-null   float64\n",
      " 5   UserLongitude       2000 non-null   float64\n",
      " 6   VmProcessingSpeed   2000 non-null   float64\n",
      " 7   VmRam               2000 non-null   int64  \n",
      " 8   VmBandwidth         2000 non-null   int64  \n",
      " 9   VmStorage           2000 non-null   int64  \n",
      " 10  VmMemoryCost        2000 non-null   float64\n",
      " 11  VmStorageCost       2000 non-null   float64\n",
      " 12  VmBandwidthCost     2000 non-null   float64\n",
      " 13  VmProcessingCost    2000 non-null   float64\n",
      " 14  CpuTime             2000 non-null   float64\n",
      " 15  TotalLength         2000 non-null   int64  \n",
      " 16  CostPerSec          2000 non-null   float64\n",
      " 17  StartExecTime       2000 non-null   float64\n",
      " 18  SerivesLevel        2000 non-null   int64  \n",
      " 19  VmID                2000 non-null   int64  \n",
      "dtypes: float64(10), int64(10)\n",
      "memory usage: 312.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/vms_scheduling_dataset_2000.csv') \n",
    "unique_labels = np.unique(df['VmID'])\n",
    "num_classes=len(unique_labels)\n",
    "print(unique_labels)\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df2b53ca-a1d7-42b6-aab8-54008d08a361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "Index(['TaskID', 'TaskFileSize', 'TaskOutputFileSize', 'TaskFileLength',\n",
      "       'CpuTime', 'TotalLength', 'VmID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "task_columns=['CostPerSec', 'StartExecTime','SerivesLevel']\n",
    "location_columns=['UserLatitude', 'UserLongitude']\n",
    "vms_columns=['VmProcessingSpeed','VmRam','VmBandwidth','VmStorage','VmMemoryCost','VmStorageCost','VmBandwidthCost','VmProcessingCost']\n",
    "\n",
    "df.drop(columns=vms_columns+task_columns+location_columns, inplace=True)\n",
    "\n",
    "X = df.drop('VmID', axis=1)  # Features\n",
    "y=df['VmID']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(np.unique(y))\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbd6c389-d52c-43bc-bcae-82279ca62002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xtrain  (1600, 1, 6) \n",
      " ytrain  (1600,) \n",
      " xtest   (400, 1, 6) \n",
      " ytest  (400,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (len(X_train.shape)<3):\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(\" xtrain \", X_train.shape, \"\\n\", \"ytrain \", y_train.shape, \"\\n\", \"xtest  \", X_test.shape, \"\\n\", \"ytest \", y_test.shape, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8a54f-e3f1-4480-a5d2-49b029b67855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/50 [..............................] - ETA: 3:18 - loss: 2.6572 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "def trainModel(hyperparameters):\n",
    "    # Build and train the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=int(hyperparameters['units']), input_shape=(1, X_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(hyperparameters['dropout_rate']))\n",
    "    model.add(LSTM(units=int(hyperparameters['units']) // 2))\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=int(hyperparameters['batch_size']),\n",
    "                        validation_data=(X_test, y_test),\n",
    "                       )\n",
    "    return model,hist\n",
    "\n",
    "SNAKE_hyperparameters = {'units': 128, 'dropout_rate': 0.01, 'learning_rate': 0.001, 'batch_size': 32}\n",
    "\n",
    "snake_model,snake_hist= trainModel(Best_Hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e083e9-b485-44f5-b3b2-79c7794ea0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snake_accuracy = max(snake_hist.history[\"accuracy\"])\n",
    "\n",
    "print(\"snake  Accuracy: \",np.round(snake_accuracy* 100) ,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62790ad1-9cc1-4fcc-b760-f887e001ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snake_model.save('../models/snake_vms_scheduling_86.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe27da-d8f6-4d35-b1ab-a1502058898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620fa73-99c2-408a-8759-0c9dea6233e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cloudsim",
   "language": "python",
   "name": ".cloudsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
